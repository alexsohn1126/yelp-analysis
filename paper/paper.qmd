---
title: "Cuisine and Yelp Ratings"
author: 
  - Moohaeng Sohn
thanks: "Code and data are available at: [https://github.com/alexsohn1126/yelp-analysis](https://github.com/alexsohn1126/yelp-analysis)"
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
toc: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(modelsummary)
library(arrow)
library(kableExtra)
library(here)
library(rstanarm)
restaurants <- read_parquet(here("./data/analysis_data/restaurant_data.parquet"))
model <- readRDS(here("./models/restaurant_rating_model_logit.rds"))
```


# Introduction

Thanks to internet technology, we can get hundreds, if not thousands of reviews of restaurants around us. This means we can choose which restaurants we will go to based on those reviews. Therefore, keeping a high review rating is very important for a restaurant's long-term success.

Yelp is a website where people can post reviews about local businesses, containing 287 million reviews [@yelpfacts]. Yelp's reviews are based on the 5-star system. Users can choose between one to five stars to put on their review. These reviews are collected and averaged, which becomes the rating for the establishment. We will use a dataset from Yelp which we will dive into in @sec-data.

TODO: ADD THINGS ABOUT WHAT WE FOUND OUT AND WHY IS IT IMPORTANT

In this paper, our estimand of interest is how different factors such as the cuisine of the restaurant, or the price of the menu items in the restaurants affect the rating of a restaurant. We will first explore the dataset in @sec-data, and discuss the model the relationship between aforementioned variables using logistic regression in @sec-model. Then we will look at the results in @sec-res, finally discussing about these results in @sec-discussion.

We used the programming language R [@citeR], along with packages `tidyverse` [@tidyverse], `rstanarm` [@rstanarm], `jsonlite` [@jsonlite], `arrow` [@arrow], `modelsummary` [@modelsummary], `kableExtra` [@kableExtra], `here` [@here].

# Data {#sec-data}

The restaurant review dataset we will use is from Yelp. Yelp offers an academic dataset for the public to use, although it is a small subset of their massive database [@yelpdataset-faq]. The dataset is split between multiple JSON files, but we will only focus on businesses JSON. The raw businesses dataset contains 150,346 businesses. These businesses are from metropolitan areas in United States and Canada. Specifically, from metropolitan areas near Montreal, Calgary, Toronto, Pittsburgh, Charlotte, Urbana-Champaign, Phoenix, Las Vegas, Madison, and Cleveland [@yelpdataset-faq].

Another possible source of restaurant reviews could have been from Google reviews of restaurants, but that requires us to use Google Business Profile APIs, which cost money. There is always an option to perform webscraping, but this is a gray area legally, and may be computationally expensive. There are other review websites such as TripAdvisor, but they do not seem to have a dataset open to the public like Yelp does.

The dataset contains multiple variables such as the name of the business, the location of the business, and things such as amenities on site. We have filtered through the raw dataset to only contain restaurants. There were a total of 47,231 restaurants in the final dataset. We will focus on the star rating, categories, and price range of the menu in the restaurant. 

## Star Rating

Star rating is the average star rating of a restaurant. One odd thing about the star rating given in the dataset is that it is rounded to the nearest star or half of a star. So all possible values of star ratings are: 1.0, 1.5, 2.0, and so on until 5.0. Notice that the minimum star rating is one and the maximum star rating is five. This is because the minimum star rating that someone can give a restaurant is 1 stars, and the maximum star rating is 5 stars. While the average star rating can have 0.5 star increments, users' reviews can only give ratings in 1 star increments. For example, no 4.5 stars can be given in a review. Each user can rate a business only once, though they are free to change their rating later, one user cannot post multiple reviews about a restaurant. This limits the power each user has to change the rating of a business.

Yelp has removed businesses that had 3 or less reviews posted that were older than 14 days at the date of collection in the dataset [@yelpdataset-faq]. This means restaurants that just opened, or didn't have enough reviews at the time of data collection would not have been included in this dataset. 

@Fig-star-rating-distribution shows us that 4.0 is the most common star rating of restaurants. This distribution is left skewed, as we can see the rating gradually increases from 1.0 to 4.0, then quickly drops off from 4.0 to 5.0. We can infer from this that most people consider somewhere around 4-stars an average dining experience. 

```{r}
#| label: fig-star-rating-distribution
#| fig-cap: Distribution of Star Ratings
#| echo: false

restaurants %>%
  ggplot(aes(x = rating)) +
  geom_histogram(binwidth = 0.25, fill = "grey", color = "black") +
  labs(title = "Distribution of Star Ratings",
       x = "Stars",
       y = "Frequency") +
  theme_minimal()
```

```{r}
#| label: tbl-star-rating-summary
#| tbl-cap: "Summary Statistics of Star Ratings"
#| echo: false

summary_stats <- restaurants %>%
  summarize(
    Count = n(),
    Mean = mean(rating),
    Median = median(rating),
    SD = sd(rating),
    Minimum = min(rating),
    Maximum = max(rating)
  )

# Create a table using kable
summary_table <- kable(summary_stats)

summary_table
```

## Categories (Cuisine)

Categories for a restaurant describes what kind of business it is. These categories are chosen manually by the business owners [@yelpcategory]. Because Yelp is a business review platform, we have decided to filter out non-restaurant businesses from our final dataset. This meant any restaurants which didn't include one of the categories: "Restaurants", "Food", "Fast Food" would not be included in the final dataset. After filtering out non-restaurants, we have chosen top 12 cuisines to categorize each restaurant into. The restaurants that did not have these cuisines in their categories were put into "Other" category. This is why we have named this section also cuisine, as we will focus on cuisine categories of restaurants.

```{r}
#| label: tbl-category-summary
#| tbl-cap: "Numbers of Restaurants Per Cuisine"
#| echo: false

# Calculate the count of restaurants per category
restaurant_summary <- restaurants %>%
  group_by(cuisine) %>%
  summarize(restaurants_count = n()) %>%
  ungroup()

# Calculate the percentage of restaurants per category
total_restaurants <- nrow(restaurants)
restaurant_summary <- restaurant_summary %>%
  mutate(percentage = round((restaurants_count / total_restaurants) * 100, 2))

# Arrange the result by frequency in descending order
restaurant_summary <- restaurant_summary %>%
  arrange(desc(restaurants_count))

# Rename the columns
colnames(restaurant_summary) <- c("Cuisine", "Number of Restaurants", "Percentage (%)")

# Print table
kable(restaurant_summary)
```

## Price Range



```{r}
#| label: fig-bills
#| fig-cap: Bills of penguins
#| echo: false
#| warning: false
#| message: false

data_counts <- restaurants %>%
  group_by(cuisine, price) %>%
  summarize(count = n()) %>%
  ungroup()

# Calculate the total count of each cuisine category
cuisine_total_counts <- data_counts %>%
  group_by(cuisine) %>%
  summarize(total_count = sum(count))

# Merge the counts with the total counts
data_normalized <- data_counts %>%
  left_join(cuisine_total_counts, by = "cuisine") %>%
  mutate(normalized_count = count / total_count)

# Plot the relationship between cuisine and price range with normalized counts
data_normalized %>%
  ggplot(aes(x = cuisine, y = normalized_count, fill = factor(price))) +
  geom_bar(stat = "identity") +
  labs(title = "Relationship between Cuisine and Price Range (Normalized)",
       x = "Cuisine",
       y = "Percentage",
       fill = "Price Range") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results {#sec-res}

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false


```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

```




# Discussion {#sec-discussion}

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

```



\newpage


# References


